{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amirmotefaker/bank-customer-churn-prediction?scriptVersionId=143949583\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Bank Customer Churn Prediction\n\n### What is churn analytics?\n- Churn analytics is the process of measuring and understanding the rate at which customers quit the product, site, or service. \n- Churn analytics can help you understand how frequently customers churn out of the product and where this tends to occur. \n- Help you understand which features and functionality are important for keeping customers in your product. \n- Churn analytics is critical for getting a performance overview, identifying improvements and understanding which channels are driving the most value.\n\n### Customer Churn Prediction - Bank\n\n- Financial institutions have many clients close their accounts or migrate to other institutions.As a result, this has made a significant hole in sales and may significantly impact yearly revenues for the current fiscal year, leading stocks to plummet and market value to fall by a decent percentage.\n\n- The objective of this project is that we want to build a model to predict, with reasonable accuracy, the customers who are going to churn soon.\n\n- A customer having closed all their active accounts with the bank is said to have churned. Churn can be defined in other ways as well, based on the context of the problem. A customer not transacting for six months or one year can also be defined as churned based on the business requirements.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport subprocess  # Subprocess in Python is a module used to run new codes and applications by creating new processes.\nimport joblib  # Lightweight pipelining with Python functions\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, recall_score, confusion_matrix, classification_report \n\n# A fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART) framework \n# based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\nfrom lightgbm import LGBMClassifier\n\n# Get multiple outputs in the same cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:37.077665Z","iopub.execute_input":"2023-09-23T09:03:37.07882Z","iopub.status.idle":"2023-09-23T09:03:39.787467Z","shell.execute_reply.started":"2023-09-23T09:03:37.078715Z","shell.execute_reply":"2023-09-23T09:03:39.786387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the DataSet","metadata":{}},{"cell_type":"code","source":"dc = pd.read_csv(\"/kaggle/input/churn-modeling-bank/Churn_Modelling.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:39.790064Z","iopub.execute_input":"2023-09-23T09:03:39.79092Z","iopub.status.idle":"2023-09-23T09:03:39.849016Z","shell.execute_reply.started":"2023-09-23T09:03:39.790866Z","shell.execute_reply":"2023-09-23T09:03:39.847936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dc.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:39.853348Z","iopub.execute_input":"2023-09-23T09:03:39.856311Z","iopub.status.idle":"2023-09-23T09:03:39.885764Z","shell.execute_reply.started":"2023-09-23T09:03:39.85625Z","shell.execute_reply":"2023-09-23T09:03:39.884438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Dimension of the dataset\ndc.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:39.891068Z","iopub.execute_input":"2023-09-23T09:03:39.893594Z","iopub.status.idle":"2023-09-23T09:03:39.902473Z","shell.execute_reply.started":"2023-09-23T09:03:39.893532Z","shell.execute_reply":"2023-09-23T09:03:39.901014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describe all numerical columns\ndc.describe(exclude= ['O'])","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:39.904748Z","iopub.execute_input":"2023-09-23T09:03:39.905336Z","iopub.status.idle":"2023-09-23T09:03:39.977732Z","shell.execute_reply.started":"2023-09-23T09:03:39.905285Z","shell.execute_reply":"2023-09-23T09:03:39.976784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describe all categorical columns\ndc.describe(include = ['O'])","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:39.978871Z","iopub.execute_input":"2023-09-23T09:03:39.97943Z","iopub.status.idle":"2023-09-23T09:03:40.011752Z","shell.execute_reply.started":"2023-09-23T09:03:39.979396Z","shell.execute_reply":"2023-09-23T09:03:40.010092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking number of unique customers in the dataset\ndc.shape[0], dc.CustomerId.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:40.013832Z","iopub.execute_input":"2023-09-23T09:03:40.014279Z","iopub.status.idle":"2023-09-23T09:03:40.024221Z","shell.execute_reply.started":"2023-09-23T09:03:40.014239Z","shell.execute_reply":"2023-09-23T09:03:40.022764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This means each row corresponds to a customer.","metadata":{}},{"cell_type":"markdown","source":"### Group by Surname to see the average churn value:","metadata":{}},{"cell_type":"code","source":"dc.groupby(['Surname']).agg({'RowNumber':'count', 'Exited':'mean'}\n                                  ).reset_index().sort_values(by='RowNumber', ascending=False).head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:40.026379Z","iopub.execute_input":"2023-09-23T09:03:40.026829Z","iopub.status.idle":"2023-09-23T09:03:40.056899Z","shell.execute_reply.started":"2023-09-23T09:03:40.026773Z","shell.execute_reply":"2023-09-23T09:03:40.055369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Group by Geography:","metadata":{}},{"cell_type":"code","source":"dc.groupby(['Geography']).agg({'RowNumber':'count', 'Exited':'mean'}\n                                  ).reset_index().sort_values(by='RowNumber', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:40.05881Z","iopub.execute_input":"2023-09-23T09:03:40.060027Z","iopub.status.idle":"2023-09-23T09:03:40.084759Z","shell.execute_reply.started":"2023-09-23T09:03:40.059966Z","shell.execute_reply":"2023-09-23T09:03:40.083192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From what we see above, customers from \"Germany\" have a higher exiting rate than average.","metadata":{}},{"cell_type":"markdown","source":"## Univariate Plots of Numerical Variables","metadata":{}},{"cell_type":"code","source":"# Plotting CreditScore\nsns.set(style=\"whitegrid\")\nsns.boxplot(y=dc['CreditScore'])","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:40.090745Z","iopub.execute_input":"2023-09-23T09:03:40.091139Z","iopub.status.idle":"2023-09-23T09:03:40.329622Z","shell.execute_reply.started":"2023-09-23T09:03:40.091104Z","shell.execute_reply":"2023-09-23T09:03:40.328344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Age\nsns.boxplot(y=dc['Age'])","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:40.331599Z","iopub.execute_input":"2023-09-23T09:03:40.332389Z","iopub.status.idle":"2023-09-23T09:03:40.539659Z","shell.execute_reply.started":"2023-09-23T09:03:40.332337Z","shell.execute_reply":"2023-09-23T09:03:40.538395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tenure violin plot\nsns.violinplot(y = dc.Tenure)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:40.541204Z","iopub.execute_input":"2023-09-23T09:03:40.54159Z","iopub.status.idle":"2023-09-23T09:03:40.782343Z","shell.execute_reply.started":"2023-09-23T09:03:40.541555Z","shell.execute_reply":"2023-09-23T09:03:40.780888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Balance violin plot\nsns.violinplot(y = dc['Balance'])","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:40.784537Z","iopub.execute_input":"2023-09-23T09:03:40.785396Z","iopub.status.idle":"2023-09-23T09:03:41.015616Z","shell.execute_reply.started":"2023-09-23T09:03:40.78534Z","shell.execute_reply":"2023-09-23T09:03:41.013524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting a distribution plot of NumOfProducts\nsns.set(style = 'ticks')\nsns.distplot(dc.NumOfProducts, hist=True, kde=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.017495Z","iopub.execute_input":"2023-09-23T09:03:41.0179Z","iopub.status.idle":"2023-09-23T09:03:41.346191Z","shell.execute_reply.started":"2023-09-23T09:03:41.017865Z","shell.execute_reply":"2023-09-23T09:03:41.345209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Most of the customers have 1 or 2 products.","metadata":{}},{"cell_type":"code","source":"# Kernel density estimation plot for EstimatedSalary\n# When dealing with numerical characteristics, one of the most useful statistics to examine is the data distribution.\n# we can use Kernel-Density-Estimation plot for that purpose. \nsns.kdeplot(dc.EstimatedSalary)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.347821Z","iopub.execute_input":"2023-09-23T09:03:41.349341Z","iopub.status.idle":"2023-09-23T09:03:41.619263Z","shell.execute_reply.started":"2023-09-23T09:03:41.349288Z","shell.execute_reply":"2023-09-23T09:03:41.618042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing\n- Tips:\n\n    - We will discard the RowNumber column.\n    - We will discard CustomerID as well since it doesn't convey any extra info. Each row pertains to a unique customer.\n    - Features can be segregated into non-essential, numerical, categorical, and target variables based on the above.\n    - CustomerID is a handy feature based on which we can calculate many user-centric features.","metadata":{}},{"cell_type":"code","source":"# Separating out different columns into various categories as defined above\ntarget_var = ['Exited']\ncols_to_remove = ['RowNumber', 'CustomerId']\n\n# numerical columns\nnum_feats = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n\n# categorical columns\ncat_feats = ['Surname', 'Geography', 'Gender', 'HasCrCard', 'IsActiveMember']","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.620939Z","iopub.execute_input":"2023-09-23T09:03:41.621373Z","iopub.status.idle":"2023-09-23T09:03:41.628295Z","shell.execute_reply.started":"2023-09-23T09:03:41.621328Z","shell.execute_reply":"2023-09-23T09:03:41.627009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Tenure and NumOfProducts are ordinal variables.\n- HasCrCard and IsActiveMember are binary categorical variables.","metadata":{}},{"cell_type":"code","source":"# Separating target variable and removing the non-essential columns\ny = dc[target_var].values\ndc.drop(cols_to_remove, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.62976Z","iopub.execute_input":"2023-09-23T09:03:41.630137Z","iopub.status.idle":"2023-09-23T09:03:41.64294Z","shell.execute_reply.started":"2023-09-23T09:03:41.630102Z","shell.execute_reply":"2023-09-23T09:03:41.641864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting Dataset\n- We keep aside a test set to evaluate our model at the very end to estimate our chosen model's performance on unseen data.\n- A validation set is also created, which we'll use in our baseline models to evaluate and tune our models.","metadata":{}},{"cell_type":"code","source":"# Keeping aside a test/holdout set\ndc_train_val, dc_test, y_train_val, y_test = train_test_split(dc, y.ravel(), test_size = 0.1, random_state = 42)\n\n# Splitting into train and validation set\ndc_train, dc_val, y_train, y_val = train_test_split(dc_train_val, y_train_val, test_size = 0.12, random_state = 42)\ndc_train.shape, dc_val.shape, dc_test.shape, y_train.shape, y_val.shape, y_test.shape\nnp.mean(y_train), np.mean(y_val), np.mean(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.644587Z","iopub.execute_input":"2023-09-23T09:03:41.645291Z","iopub.status.idle":"2023-09-23T09:03:41.680147Z","shell.execute_reply.started":"2023-09-23T09:03:41.645251Z","shell.execute_reply":"2023-09-23T09:03:41.678478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Variable Encoding\n1- Label Encoding: transforms non-numerical labels into numerical ones. It can be used for binary categorical and ordinal variables.\n\n2- One-Hot encoding: encodes categorical features as a one-hot numeric array. It can be used for non-ordinal categorical variables with low to mid cardinality (< 5-10 levels)\n\n3- Target encoding: the technique of substituting a categorical value with the mean of the target variable is known as target encoding. The target encoder model automatically removes any non-categorical columns. It can be used for Categorical variables with > 10 levels\n\n    - HasCrCard and IsActiveMember are already label encoded.\n\n    - Gender, a simple label encoding should be acceptable.\n\n    - Geography, since there are three levels, one-hot encoding should do the trick.\n\n    - Surname, we'll try target/frequency encoding.","metadata":{}},{"cell_type":"markdown","source":"## 1- Label Encoding","metadata":{}},{"cell_type":"markdown","source":"### Label Encoding for Binary Variables","metadata":{}},{"cell_type":"markdown","source":"- Label encoding on the Gender column.\n","metadata":{}},{"cell_type":"code","source":"# label encoding With  the sklearn method\nle = LabelEncoder()\n# Label encoding of Gender variable\ndc_train['Gender'] = le.fit_transform(dc_train['Gender'])\nle_gender_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nle_gender_mapping","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.682718Z","iopub.execute_input":"2023-09-23T09:03:41.6837Z","iopub.status.idle":"2023-09-23T09:03:41.721652Z","shell.execute_reply.started":"2023-09-23T09:03:41.683617Z","shell.execute_reply":"2023-09-23T09:03:41.71976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing and validation sets","metadata":{}},{"cell_type":"code","source":"# Encoding Gender feature for validation and test set\ndc_val['Gender'] = dc_val.Gender.map(le_gender_mapping)\ndc_test['Gender'] = dc_test.Gender.map(le_gender_mapping)\n\n# Filling missing/NaN values created due to new categorical levels\ndc_val['Gender'].fillna(-1, inplace=True)\ndc_test['Gender'].fillna(-1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.724408Z","iopub.execute_input":"2023-09-23T09:03:41.727098Z","iopub.status.idle":"2023-09-23T09:03:41.75871Z","shell.execute_reply.started":"2023-09-23T09:03:41.727021Z","shell.execute_reply":"2023-09-23T09:03:41.756343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking the values on all sets","metadata":{}},{"cell_type":"code","source":"dc_train.Gender.unique(), dc_val.Gender.unique(), dc_test.Gender.unique()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.762194Z","iopub.execute_input":"2023-09-23T09:03:41.764869Z","iopub.status.idle":"2023-09-23T09:03:41.786816Z","shell.execute_reply.started":"2023-09-23T09:03:41.76479Z","shell.execute_reply":"2023-09-23T09:03:41.785649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2- One-Hot encoding","metadata":{}},{"cell_type":"markdown","source":"### One-hot Encoding Categorical Variables","metadata":{}},{"cell_type":"code","source":"# one-hot encode the Geography column\n\n# With the sklearn method(LabelEncoder())\nle_ohe = LabelEncoder()\nohe = OneHotEncoder(handle_unknown = 'ignore', sparse=False)\nenc_train = le_ohe.fit_transform(dc_train.Geography).reshape(dc_train.shape[0],1)\nohe_train = ohe.fit_transform(enc_train)\nohe_train","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.788728Z","iopub.execute_input":"2023-09-23T09:03:41.789236Z","iopub.status.idle":"2023-09-23T09:03:41.81104Z","shell.execute_reply.started":"2023-09-23T09:03:41.789168Z","shell.execute_reply":"2023-09-23T09:03:41.809832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mapping between classes\nle_ohe_geography_mapping = dict(zip(le_ohe.classes_, le_ohe.transform(le_ohe.classes_)))\nle_ohe_geography_mapping","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.813151Z","iopub.execute_input":"2023-09-23T09:03:41.814445Z","iopub.status.idle":"2023-09-23T09:03:41.824368Z","shell.execute_reply.started":"2023-09-23T09:03:41.81439Z","shell.execute_reply":"2023-09-23T09:03:41.822995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing and validation sets","metadata":{}},{"cell_type":"code","source":"# Encoding Geography feature for validation and test set\nenc_val = dc_val.Geography.map(le_ohe_geography_mapping).ravel().reshape(-1,1)\nenc_test = dc_test.Geography.map(le_ohe_geography_mapping).ravel().reshape(-1,1)\n\n# Filling missing/NaN values created due to new categorical levels\nenc_val[np.isnan(enc_val)] = 9999\nenc_test[np.isnan(enc_test)] = 9999\nohe_val = ohe.transform(enc_val)\nohe_test = ohe.transform(enc_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.826078Z","iopub.execute_input":"2023-09-23T09:03:41.826508Z","iopub.status.idle":"2023-09-23T09:03:41.840803Z","shell.execute_reply.started":"2023-09-23T09:03:41.826471Z","shell.execute_reply":"2023-09-23T09:03:41.839293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- In case there is a country that isn't present in the training set, the resulting vector will simply be [0, 0, 0]:","metadata":{}},{"cell_type":"code","source":"# Show what happens when a new value is inputted into the OHE \nohe.transform(np.array([[9999]]))","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.842737Z","iopub.execute_input":"2023-09-23T09:03:41.843223Z","iopub.status.idle":"2023-09-23T09:03:41.858063Z","shell.execute_reply.started":"2023-09-23T09:03:41.843158Z","shell.execute_reply":"2023-09-23T09:03:41.856343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding the one-hot encoded columns to the data frame and removing the original feature:","metadata":{}},{"cell_type":"code","source":"cols = ['country_' + str(x) for x in le_ohe_geography_mapping.keys()]\ncols","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.859885Z","iopub.execute_input":"2023-09-23T09:03:41.860711Z","iopub.status.idle":"2023-09-23T09:03:41.868131Z","shell.execute_reply.started":"2023-09-23T09:03:41.860666Z","shell.execute_reply":"2023-09-23T09:03:41.867202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding to the respective dataframes\ndc_train = pd.concat([dc_train.reset_index(), pd.DataFrame(ohe_train, columns = cols)], axis = 1).drop(['index'], axis=1)\ndc_val = pd.concat([dc_val.reset_index(), pd.DataFrame(ohe_val, columns = cols)], axis = 1).drop(['index'], axis=1)\ndc_test = pd.concat([dc_test.reset_index(), pd.DataFrame(ohe_test, columns = cols)], axis = 1).drop(['index'], axis=1)\nprint(\"Training set\")\ndc_train.head()\nprint(\"\\n\\nValidation set\")\ndc_val.head()\nprint(\"\\n\\nTest set\")\ndc_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.869896Z","iopub.execute_input":"2023-09-23T09:03:41.870617Z","iopub.status.idle":"2023-09-23T09:03:41.958225Z","shell.execute_reply.started":"2023-09-23T09:03:41.870572Z","shell.execute_reply":"2023-09-23T09:03:41.956967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropping the original Geography column now:","metadata":{}},{"cell_type":"code","source":"dc_train.drop(['Geography'], axis=1, inplace=True)\ndc_val.drop(['Geography'], axis=1, inplace=True)\ndc_test.drop(['Geography'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.968538Z","iopub.execute_input":"2023-09-23T09:03:41.968982Z","iopub.status.idle":"2023-09-23T09:03:41.979214Z","shell.execute_reply.started":"2023-09-23T09:03:41.96894Z","shell.execute_reply":"2023-09-23T09:03:41.977956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataframes without Geography\nprint(\"Training set\")\ndc_train.head()\nprint(\"\\n\\nValidation set\")\ndc_val.head()\nprint(\"\\n\\nTest set\")\ndc_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:41.980923Z","iopub.execute_input":"2023-09-23T09:03:41.981346Z","iopub.status.idle":"2023-09-23T09:03:42.050145Z","shell.execute_reply.started":"2023-09-23T09:03:41.981302Z","shell.execute_reply":"2023-09-23T09:03:42.048794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3- Target encoding\n- Target encoding is generally proper when dealing with categorical variables of high cardinality (high number of levels).\n","metadata":{}},{"cell_type":"markdown","source":"### Encode the Surname column (which has 2932 different values!) with the mean of the target variable for that level:","metadata":{}},{"cell_type":"code","source":"means = dc_train.groupby(['Surname']).Exited.mean()\nmeans.head()\nmeans.tail()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.054013Z","iopub.execute_input":"2023-09-23T09:03:42.054816Z","iopub.status.idle":"2023-09-23T09:03:42.074386Z","shell.execute_reply.started":"2023-09-23T09:03:42.054775Z","shell.execute_reply":"2023-09-23T09:03:42.07303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Global mean of Exited column:","metadata":{}},{"cell_type":"code","source":"global_mean = y_train.mean()\nglobal_mean","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.075882Z","iopub.execute_input":"2023-09-23T09:03:42.076579Z","iopub.status.idle":"2023-09-23T09:03:42.084693Z","shell.execute_reply.started":"2023-09-23T09:03:42.076533Z","shell.execute_reply":"2023-09-23T09:03:42.083493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating new encoded features for surname - Target (mean) encoding\ndc_train['Surname_mean_churn'] = dc_train.Surname.map(means)\ndc_train['Surname_mean_churn'].fillna(global_mean, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.086324Z","iopub.execute_input":"2023-09-23T09:03:42.08672Z","iopub.status.idle":"2023-09-23T09:03:42.102773Z","shell.execute_reply.started":"2023-09-23T09:03:42.086684Z","shell.execute_reply":"2023-09-23T09:03:42.100998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The problem with target encoding is that it might cause \"data leakage\", as we are considering feedback from the target variable while computing any summary statistic.\n    - A solution is to use a modified version: Leave-one-out Target encoding; for a particular data point or row, the mean of the target is calculated by considering all rows in the same definite level except itself. It mitigates data leakage and overfitting to some extent.","metadata":{}},{"cell_type":"markdown","source":"### Calculating the frequency of each Surname:","metadata":{}},{"cell_type":"code","source":"freqs = dc_train.groupby(['Surname']).size()\nfreqs.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.10533Z","iopub.execute_input":"2023-09-23T09:03:42.10715Z","iopub.status.idle":"2023-09-23T09:03:42.134615Z","shell.execute_reply.started":"2023-09-23T09:03:42.107077Z","shell.execute_reply":"2023-09-23T09:03:42.132711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create frequency encoding - Number of instances of each category in the data:","metadata":{}},{"cell_type":"code","source":"dc_train['Surname_freq'] = dc_train.Surname.map(freqs)\ndc_train['Surname_freq'].fillna(0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.136916Z","iopub.execute_input":"2023-09-23T09:03:42.138534Z","iopub.status.idle":"2023-09-23T09:03:42.163365Z","shell.execute_reply.started":"2023-09-23T09:03:42.13812Z","shell.execute_reply":"2023-09-23T09:03:42.161013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating Leave-one-out target encoding for Surname:","metadata":{}},{"cell_type":"code","source":"dc_train['Surname_enc'] = ((dc_train.Surname_freq * dc_train.Surname_mean_churn) - dc_train.Exited)/(dc_train.Surname_freq - 1)\n\n# Fill NaNs occuring due to category frequency being 1 or less\ndc_train['Surname_enc'].fillna((((dc_train.shape[0] * global_mean) - dc_train.Exited) / (dc_train.shape[0] - 1)), inplace=True)\ndc_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.166339Z","iopub.execute_input":"2023-09-23T09:03:42.168681Z","iopub.status.idle":"2023-09-23T09:03:42.232921Z","shell.execute_reply.started":"2023-09-23T09:03:42.168582Z","shell.execute_reply":"2023-09-23T09:03:42.231793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### On the validation and testing set, we'll apply the regular target encoding mapping as obtained from the training set:","metadata":{}},{"cell_type":"code","source":"# Replacing by category means and new category levels by global mean\ndc_val['Surname_enc'] = dc_val.Surname.map(means)\ndc_val['Surname_enc'].fillna(global_mean, inplace=True)\ndc_test['Surname_enc'] = dc_test.Surname.map(means)\ndc_test['Surname_enc'].fillna(global_mean, inplace=True)\n\n# Show that using LOO Target encoding decorrelates features\ndc_train[['Surname_mean_churn', 'Surname_enc', 'Exited']].corr()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.234244Z","iopub.execute_input":"2023-09-23T09:03:42.235226Z","iopub.status.idle":"2023-09-23T09:03:42.259002Z","shell.execute_reply.started":"2023-09-23T09:03:42.235161Z","shell.execute_reply":"2023-09-23T09:03:42.25757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As you can see, \"Surname_enc\" isn't highly correlated with \"Exited\" as \"Surname_mean_churn\", so Leave-one-out is effective here.","metadata":{}},{"cell_type":"markdown","source":"### Deleting the Surname and other redundant columns across the three datasets:","metadata":{}},{"cell_type":"code","source":"dc_train.drop(['Surname_mean_churn'], axis=1, inplace=True)\ndc_train.drop(['Surname_freq'], axis=1, inplace=True)\ndc_train.drop(['Surname'], axis=1, inplace=True)\ndc_val.drop(['Surname'], axis=1, inplace=True)\ndc_test.drop(['Surname'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.260892Z","iopub.execute_input":"2023-09-23T09:03:42.261717Z","iopub.status.idle":"2023-09-23T09:03:42.276074Z","shell.execute_reply.started":"2023-09-23T09:03:42.261665Z","shell.execute_reply":"2023-09-23T09:03:42.275088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dc_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.277995Z","iopub.execute_input":"2023-09-23T09:03:42.278964Z","iopub.status.idle":"2023-09-23T09:03:42.303313Z","shell.execute_reply.started":"2023-09-23T09:03:42.278916Z","shell.execute_reply":"2023-09-23T09:03:42.30199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary:\n- Use label encoding and one-hot encoding on the training set and then save the mapping and apply it to the test set. For missing values, use 0, -1 etc.\n\n- Target/Frequency encoding: Create a mapping between each level and a statistical measure (mean, median, sum, etc.) of the target from the training dataset. For the new categorical levels, impute the missing values suitably (can be 0, -1, or mean/mode/median).\n\n- Leave-one-out or Cross-fold Target encoding avoids data leakage and helps generalize the model.","metadata":{}},{"cell_type":"markdown","source":"## Bivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"### Checking the linear correlation between individual features and the target variable:","metadata":{}},{"cell_type":"code","source":"corr = dc_train.corr()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.304922Z","iopub.execute_input":"2023-09-23T09:03:42.30537Z","iopub.status.idle":"2023-09-23T09:03:42.31746Z","shell.execute_reply.started":"2023-09-23T09:03:42.305322Z","shell.execute_reply":"2023-09-23T09:03:42.316003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drawing a heatmap\nsns.heatmap(corr, cmap = 'coolwarm')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.318925Z","iopub.execute_input":"2023-09-23T09:03:42.319721Z","iopub.status.idle":"2023-09-23T09:03:42.978527Z","shell.execute_reply.started":"2023-09-23T09:03:42.319671Z","shell.execute_reply":"2023-09-23T09:03:42.97712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- None of the features are highly correlated with the target variable. But some of them have slight linear associations with the target variable:\n\n    - Continuous features: Age, Balance.\n    - Categorical variables: Gender, IsActiveMember, country_Germany, country_France.","metadata":{}},{"cell_type":"markdown","source":"### Individual features versus their distribution across target variable values, for \"Age\" column:","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x=\"Exited\", y=\"Age\", data=dc_train, palette=\"Set3\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:42.979637Z","iopub.execute_input":"2023-09-23T09:03:42.980009Z","iopub.status.idle":"2023-09-23T09:03:43.224088Z","shell.execute_reply.started":"2023-09-23T09:03:42.979975Z","shell.execute_reply":"2023-09-23T09:03:43.223041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Tips:\n    - There is an outside point at 90 for class 0 and 83 for class 1.\n    - The Median is around 35 for class 0 and 45 for class 1.\n    - The lower adjacent value is a bit less than 20 for both classes.\n    - The upper adjoining value is around 55 for class 0 and more than 70 for class 1.","metadata":{}},{"cell_type":"markdown","source":"### Violin plot for the \"Balance\":","metadata":{}},{"cell_type":"code","source":"sns.violinplot(x=\"Exited\", y=\"Balance\", data=dc_train, palette=\"Set3\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:43.225414Z","iopub.execute_input":"2023-09-23T09:03:43.226493Z","iopub.status.idle":"2023-09-23T09:03:43.493017Z","shell.execute_reply.started":"2023-09-23T09:03:43.226451Z","shell.execute_reply":"2023-09-23T09:03:43.491759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We see a high probability of observation at 125000 (balance) and a peak at 0 (balance) for class 0.","metadata":{}},{"cell_type":"markdown","source":"### Checking the association of categorical features with the target variable:","metadata":{}},{"cell_type":"code","source":"cat_vars_bv = ['Gender', 'IsActiveMember', 'country_Germany', 'country_France']\n\nfor col in cat_vars_bv:\n    dc_train.groupby([col]).Exited.mean()\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:43.494941Z","iopub.execute_input":"2023-09-23T09:03:43.49577Z","iopub.status.idle":"2023-09-23T09:03:43.540372Z","shell.execute_reply.started":"2023-09-23T09:03:43.49572Z","shell.execute_reply":"2023-09-23T09:03:43.538862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above results, we can observe that the mean of the churned customer across \"Germany\" is higher than the others.","metadata":{}},{"cell_type":"markdown","source":"### Computed mean on churned or non chuned custmers group by number of product on training data","metadata":{}},{"cell_type":"code","source":"col = 'NumOfProducts'\ndc_train.groupby([col]).Exited.mean()\n\n# unique \"NumOfProducts\" on training data\ndc_train[col].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:43.54207Z","iopub.execute_input":"2023-09-23T09:03:43.54259Z","iopub.status.idle":"2023-09-23T09:03:43.562794Z","shell.execute_reply.started":"2023-09-23T09:03:43.542543Z","shell.execute_reply":"2023-09-23T09:03:43.561459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- All customers with four products have churned.\n- About 82.5% of customers with three products have churned.","metadata":{}},{"cell_type":"markdown","source":"### Creating some new features based on simple interactions between the existing features:","metadata":{}},{"cell_type":"code","source":"eps = 1e-6\n\ndc_train['bal_per_product'] = dc_train.Balance/(dc_train.NumOfProducts + eps)\ndc_train['bal_by_est_salary'] = dc_train.Balance/(dc_train.EstimatedSalary + eps)\ndc_train['tenure_age_ratio'] = dc_train.Tenure/(dc_train.Age + eps)\ndc_train['age_surname_mean_churn'] = np.sqrt(dc_train.Age) * dc_train.Surname_enc","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:43.564702Z","iopub.execute_input":"2023-09-23T09:03:43.566357Z","iopub.status.idle":"2023-09-23T09:03:43.580826Z","shell.execute_reply.started":"2023-09-23T09:03:43.566303Z","shell.execute_reply":"2023-09-23T09:03:43.579615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_cols = ['bal_per_product', 'bal_by_est_salary', 'tenure_age_ratio', 'age_surname_mean_churn']\n\n# Ensuring that the new column doesn't have any missing values\ndc_train[new_cols].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:43.582724Z","iopub.execute_input":"2023-09-23T09:03:43.583604Z","iopub.status.idle":"2023-09-23T09:03:43.599149Z","shell.execute_reply.started":"2023-09-23T09:03:43.583549Z","shell.execute_reply":"2023-09-23T09:03:43.597632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear association of new columns with target variables to judge importance:\n","metadata":{}},{"cell_type":"code","source":"sns.heatmap(dc_train[new_cols + ['Exited']].corr(), annot=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:43.601494Z","iopub.execute_input":"2023-09-23T09:03:43.602546Z","iopub.status.idle":"2023-09-23T09:03:44.044931Z","shell.execute_reply.started":"2023-09-23T09:03:43.602493Z","shell.execute_reply":"2023-09-23T09:03:44.043621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Out of the new features, ones with slight linear association/correlation are \"bal_per_product\" and \"tenure_age_ratio\".","metadata":{}},{"cell_type":"markdown","source":"### Creating them on the validation and testing sets:","metadata":{}},{"cell_type":"code","source":"dc_val['bal_per_product'] = dc_val.Balance/(dc_val.NumOfProducts + eps)\ndc_val['bal_by_est_salary'] = dc_val.Balance/(dc_val.EstimatedSalary + eps)\ndc_val['tenure_age_ratio'] = dc_val.Tenure/(dc_val.Age + eps)\ndc_val['age_surname_mean_churn'] = np.sqrt(dc_val.Age) * dc_val.Surname_enc\ndc_test['bal_per_product'] = dc_test.Balance/(dc_test.NumOfProducts + eps)\ndc_test['bal_by_est_salary'] = dc_test.Balance/(dc_test.EstimatedSalary + eps)\ndc_test['tenure_age_ratio'] = dc_test.Tenure/(dc_test.Age + eps)\ndc_test['age_surname_mean_churn'] = np.sqrt(dc_test.Age) * dc_test.Surname_enc","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:44.046354Z","iopub.execute_input":"2023-09-23T09:03:44.046736Z","iopub.status.idle":"2023-09-23T09:03:44.071324Z","shell.execute_reply.started":"2023-09-23T09:03:44.046701Z","shell.execute_reply":"2023-09-23T09:03:44.069933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling and Normalization","metadata":{}},{"cell_type":"markdown","source":"### Scale the continuous columns with the \"StandardScaler\":","metadata":{}},{"cell_type":"code","source":"# initialize the standard scaler\nsc = StandardScaler()\ncont_vars = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'Surname_enc', 'bal_per_product'\n             , 'bal_by_est_salary', 'tenure_age_ratio', 'age_surname_mean_churn']\ncat_vars = ['Gender', 'HasCrCard', 'IsActiveMember', 'country_France', 'country_Germany', 'country_Spain']\n\n# Scaling only continuous columns\ncols_to_scale = cont_vars\nsc_X_train = sc.fit_transform(dc_train[cols_to_scale])\n\n# Converting from array to dataframe and naming the respective features/columns\nsc_X_train = pd.DataFrame(data=sc_X_train, columns=cols_to_scale)\nsc_X_train.shape\nsc_X_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:44.074199Z","iopub.execute_input":"2023-09-23T09:03:44.074768Z","iopub.status.idle":"2023-09-23T09:03:44.115974Z","shell.execute_reply.started":"2023-09-23T09:03:44.074717Z","shell.execute_reply":"2023-09-23T09:03:44.114947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling validation and test sets by transforming the mapping obtained through the training set\nsc_X_val = sc.transform(dc_val[cols_to_scale])\nsc_X_test = sc.transform(dc_test[cols_to_scale])\n\n# Converting val and test arrays to dataframes for re-usability\nsc_X_val = pd.DataFrame(data=sc_X_val, columns=cols_to_scale)\nsc_X_test = pd.DataFrame(data=sc_X_test, columns=cols_to_scale)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:44.117728Z","iopub.execute_input":"2023-09-23T09:03:44.118147Z","iopub.status.idle":"2023-09-23T09:03:44.134492Z","shell.execute_reply.started":"2023-09-23T09:03:44.118109Z","shell.execute_reply":"2023-09-23T09:03:44.133105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc_X_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:44.138116Z","iopub.execute_input":"2023-09-23T09:03:44.138513Z","iopub.status.idle":"2023-09-23T09:03:44.165159Z","shell.execute_reply.started":"2023-09-23T09:03:44.13848Z","shell.execute_reply":"2023-09-23T09:03:44.163629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection using RFE\n\n- Feature scaling is essential for algorithms like Logistic Regression and SVM.\n- Feature scaling is not necessarily crucial for tree-based models such as decision trees.\n\n- We can perform feature selection by eliminating features from the training dataset through Recursive Feature Elimination (RFE).\n- Features shortlisted through EDA/manual inspection and bivariate analysis:\n\n    - Age, Gender, Balance, NumOfProducts, IsActiveMember, the three Geography variables, bal_per_product, tenure_age_ratio.\n    \n- let's see whether feature selection and elimination through RFE give us the same list of features, other extra features, or a lesser number of features.","metadata":{}},{"cell_type":"markdown","source":"### To begin with, we'll feed all features to the RFE and LogReg model:","metadata":{}},{"cell_type":"code","source":"# Creating feature-set and target for RFE model\ny = dc_train['Exited'].values\nX = dc_train[cat_vars + cont_vars]\nX.columns = cat_vars + cont_vars\nX.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:44.167071Z","iopub.execute_input":"2023-09-23T09:03:44.167842Z","iopub.status.idle":"2023-09-23T09:03:44.178287Z","shell.execute_reply.started":"2023-09-23T09:03:44.1678Z","shell.execute_reply":"2023-09-23T09:03:44.177318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We want to select the features (you can change that if you wish to), so we set n_features_to_select as 10:","metadata":{}},{"cell_type":"code","source":"# for logistics regression\nrfe = RFE(estimator=LogisticRegression(), n_features_to_select=10) \nrfe = rfe.fit(X.values, y) \n\n# mask of selected features\nprint(rfe.support_)\n# The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature \nprint(rfe.ranking_)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:44.179831Z","iopub.execute_input":"2023-09-23T09:03:44.180242Z","iopub.status.idle":"2023-09-23T09:03:45.762422Z","shell.execute_reply.started":"2023-09-23T09:03:44.180198Z","shell.execute_reply":"2023-09-23T09:03:45.76069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Printed the selected features as well as the ranking of the feature by importance.","metadata":{}},{"cell_type":"markdown","source":"### Logistic regression (linear)","metadata":{}},{"cell_type":"code","source":"mask = rfe.support_.tolist()\nselected_feats = [b for a,b in zip(mask, X.columns) if a]\nselected_feats","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:45.764703Z","iopub.execute_input":"2023-09-23T09:03:45.765771Z","iopub.status.idle":"2023-09-23T09:03:45.78049Z","shell.execute_reply.started":"2023-09-23T09:03:45.765702Z","shell.execute_reply":"2023-09-23T09:03:45.778746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Features that were chosen for logistic regression.","metadata":{}},{"cell_type":"code","source":"rfe_dt = RFE(estimator=DecisionTreeClassifier(max_depth = 4, criterion = 'entropy'), n_features_to_select=10) \nrfe_dt = rfe_dt.fit(X.values, y)  \nmask = rfe_dt.support_.tolist()\nselected_feats_dt = [b for a,b in zip(mask, X.columns) if a]\nselected_feats_dt","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:45.783062Z","iopub.execute_input":"2023-09-23T09:03:45.784149Z","iopub.status.idle":"2023-09-23T09:03:46.098683Z","shell.execute_reply.started":"2023-09-23T09:03:45.784073Z","shell.execute_reply":"2023-09-23T09:03:46.09728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Features that were chosen for the decision tree model.","metadata":{}},{"cell_type":"markdown","source":"## Baseline Model 1: Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"### Train the linear models on the features selected with RFE:","metadata":{}},{"cell_type":"code","source":"selected_cat_vars = [x for x in selected_feats if x in cat_vars]\nselected_cont_vars = [x for x in selected_feats if x in cont_vars]\n\n# Using categorical features and scaled numerical features\nX_train = np.concatenate((dc_train[selected_cat_vars].values, sc_X_train[selected_cont_vars].values), axis=1)\nX_val = np.concatenate((dc_val[selected_cat_vars].values, sc_X_val[selected_cont_vars].values), axis=1)\nX_test = np.concatenate((dc_test[selected_cat_vars].values, sc_X_test[selected_cont_vars].values), axis=1)\n\n# print the shapes\nX_train.shape, X_val.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:46.100012Z","iopub.execute_input":"2023-09-23T09:03:46.100408Z","iopub.status.idle":"2023-09-23T09:03:46.121558Z","shell.execute_reply.started":"2023-09-23T09:03:46.100372Z","shell.execute_reply":"2023-09-23T09:03:46.120002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- During the training phase, the weight differential will affect the classification of the classes.","metadata":{}},{"cell_type":"markdown","source":"### The overall goal is to penalize the minority class for misclassification by increasing class weight while decreasing weight for the majority class:","metadata":{}},{"cell_type":"code","source":"# Obtaining class weights based on the class samples imbalance ratio\n_, num_samples = np.unique(y_train, return_counts=True)\nweights = np.max(num_samples)/num_samples\n\n# Define weight dictionnary\nweights_dict = dict()\nclass_labels = [0,1]\n\n# Weights associated with classes\nfor a,b in zip(class_labels,weights):\n    weights_dict[a] = b\n\nweights_dict","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:46.123336Z","iopub.execute_input":"2023-09-23T09:03:46.123719Z","iopub.status.idle":"2023-09-23T09:03:46.135278Z","shell.execute_reply.started":"2023-09-23T09:03:46.123684Z","shell.execute_reply":"2023-09-23T09:03:46.133989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Retrain with class_weight:","metadata":{}},{"cell_type":"code","source":"# Defining model\nlr = LogisticRegression(C=1.0, penalty='l2', class_weight=weights_dict, n_jobs=-1)\n\n# train\nlr.fit(X_train, y_train)\nprint(f'Confusion Matrix: \\n{confusion_matrix(y_val, lr.predict(X_val))}')\nprint(f'Area Under Curve: {roc_auc_score(y_val, lr.predict(X_val))}')\nprint(f'Recall score: {recall_score(y_val,lr.predict(X_val))}')\nprint(f'Classification report: \\n{classification_report(y_val,lr.predict(X_val))}')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:46.136819Z","iopub.execute_input":"2023-09-23T09:03:46.137236Z","iopub.status.idle":"2023-09-23T09:03:47.253667Z","shell.execute_reply.started":"2023-09-23T09:03:46.137168Z","shell.execute_reply":"2023-09-23T09:03:47.251659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline Model 2: Support Vector Machines(SVM)","metadata":{}},{"cell_type":"markdown","source":"### Train an SVC:","metadata":{}},{"cell_type":"code","source":"svm = SVC(C=1.0, kernel=\"linear\", class_weight=weights_dict)\nsvm.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:47.262774Z","iopub.execute_input":"2023-09-23T09:03:47.26783Z","iopub.status.idle":"2023-09-23T09:03:52.38691Z","shell.execute_reply.started":"2023-09-23T09:03:47.267736Z","shell.execute_reply":"2023-09-23T09:03:52.385477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note:\n- In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n- On GitHub, the HTML representation is unable to render, please try loading this page with [nbviewer](https://nbviewer.org/)","metadata":{}},{"cell_type":"markdown","source":"### Validation metrics\n","metadata":{}},{"cell_type":"code","source":"print(f'Confusion Matrix: {confusion_matrix(y_val, lr.predict(X_val))}')\nprint(f'Area Under Curve: {roc_auc_score(y_val, lr.predict(X_val))}')\nprint(f'Recall score: {recall_score(y_val,lr.predict(X_val))}')\nprint(f'Classification report: \\n{classification_report(y_val,lr.predict(X_val))}')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:52.38879Z","iopub.execute_input":"2023-09-23T09:03:52.389755Z","iopub.status.idle":"2023-09-23T09:03:52.428629Z","shell.execute_reply.started":"2023-09-23T09:03:52.389703Z","shell.execute_reply":"2023-09-23T09:03:52.426871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting Decision Boundaries of Linear Models\n- To plot the decision boundaries of classification models in a 2-D space, we first need to train our models in a 2D space.\n- The best option is to use our existing data (with > 2 features) and apply dimensionality reduction techniques (like PCA) to it and then train our models on this data with a reduced number of features.","metadata":{}},{"cell_type":"markdown","source":"- We will create an image in Meshgrid in which each pixel represents a grid cell in the 2D feature space.\n- Over the 2D feature space, the image creates a grid. The classifier is used to classify the image's pixels, assigning a class label to each grid cell.\n- The identified image is then utilized as the backdrop for a scatter plot with data points from each class.","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=2)\n\n# Transforming the dataset using PCA\nX_pca = pca.fit_transform(X_train)\ny = y_train\nX_pca.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:52.431327Z","iopub.execute_input":"2023-09-23T09:03:52.432446Z","iopub.status.idle":"2023-09-23T09:03:52.488036Z","shell.execute_reply.started":"2023-09-23T09:03:52.432372Z","shell.execute_reply":"2023-09-23T09:03:52.485944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min and max values \nxmin, xmax = X_pca[:, 0].min() - 2, X_pca[:, 0].max() + 2\nymin, ymax = X_pca[:, 1].min() - 2, X_pca[:, 1].max() + 2\n\n# Creating a mesh region where the boundary will be plotted\nxx, yy = np.meshgrid(np.arange(xmin, xmax, 0.2),\n                     np.arange(ymin, ymax, 0.2))\n\n# Fitting LR model on 2 features\nlr.fit(X_pca, y)\n\n# Fitting SVM model on 2 features\nsvm.fit(X_pca, y)\n\n# Plotting decision boundary for LR\nz1 = lr.predict(np.c_[xx.ravel(), yy.ravel()])\nz1 = z1.reshape(xx.shape)\n\n# Plotting decision boundary for SVM\nz2 = svm.predict(np.c_[xx.ravel(), yy.ravel()])\nz2 = z2.reshape(xx.shape)\n\n# Displaying the result\nplt.contourf(xx, yy, z1, alpha=0.4) # LR\nplt.contour(xx, yy, z2, alpha=0.4, colors='blue') # SVM\nsns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y_train, s=50, alpha=0.8)\nplt.title('Linear models - LogReg and SVM')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:52.492235Z","iopub.execute_input":"2023-09-23T09:03:52.495095Z","iopub.status.idle":"2023-09-23T09:03:58.538681Z","shell.execute_reply.started":"2023-09-23T09:03:52.495014Z","shell.execute_reply":"2023-09-23T09:03:58.53739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tips:\n- Logistic regression estimator uses a linear separator for decision boundary.\n- The Support Vector Machine finds a hyperplane with the maximum margin that divides the feature space into two classes.\n- Decision Dense sampling with a mesh grid may be used to illustrate decision boundary. However, the boundaries will seem erroneous if the grid resolution is insufficient. Mesh grid aims to generate a rectangular grid from an array of x values and an array of y values.","metadata":{}},{"cell_type":"markdown","source":"### Note:\n- In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n- On GitHub, the HTML representation is unable to render, please try loading this page with [nbviewer](https://nbviewer.org/)","metadata":{}},{"cell_type":"markdown","source":"## Baseline Model 3 (Non-linear): Decision Tree Classifier\n- Decision Trees are among the easiest algorithms to grasp, and their rationale is straightforward to explain.","metadata":{}},{"cell_type":"code","source":"# Features selected from the RFE process\nselected_feats_dt","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:58.540071Z","iopub.execute_input":"2023-09-23T09:03:58.541236Z","iopub.status.idle":"2023-09-23T09:03:58.549028Z","shell.execute_reply.started":"2023-09-23T09:03:58.541193Z","shell.execute_reply":"2023-09-23T09:03:58.547863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-defining X_train and X_val to consider original unscaled continuous features. y_train and y_val remain unaffected\nX_train = dc_train[selected_feats_dt].values\nX_val = dc_val[selected_feats_dt].values\n\n# Decision tree classiier model\nclf = DecisionTreeClassifier(criterion='entropy', class_weight=weights_dict, max_depth=4, max_features=None\n                            , min_samples_split=25, min_samples_leaf=15)\n\n# Fit the model\nclf.fit(X_train, y_train)\n\n# Checking the importance of different features of the model\npd.DataFrame({'features': selected_feats,\n              'importance': clf.feature_importances_\n             }).sort_values(by='importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:58.55062Z","iopub.execute_input":"2023-09-23T09:03:58.551007Z","iopub.status.idle":"2023-09-23T09:03:58.607462Z","shell.execute_reply.started":"2023-09-23T09:03:58.550972Z","shell.execute_reply":"2023-09-23T09:03:58.606056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- IsActiveMember, country_Germany, Gender are the most important features for a decision tree.","metadata":{}},{"cell_type":"markdown","source":"### Note:\n- In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n- On GitHub, the HTML representation is unable to render, please try loading this page with [nbviewer](https://nbviewer.org/)","metadata":{}},{"cell_type":"markdown","source":"### Evaluating the model on training and validation dataset:","metadata":{}},{"cell_type":"code","source":"# Validation metrics\nprint(f'Confusion Matrix: {confusion_matrix(y_val, clf.predict(X_val))}')\nprint(f'Area Under Curve: {roc_auc_score(y_val, clf.predict(X_val))}')\nprint(f'Recall score: {recall_score(y_val,clf.predict(X_val))}')\nprint(f'Classification report: \\n{classification_report(y_val,clf.predict(X_val))}')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:58.609374Z","iopub.execute_input":"2023-09-23T09:03:58.609911Z","iopub.status.idle":"2023-09-23T09:03:58.632151Z","shell.execute_reply.started":"2023-09-23T09:03:58.60986Z","shell.execute_reply":"2023-09-23T09:03:58.631231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We got better metrics here, 0.7 as a macro average of F1-score and 0.77 as a weighted average.","metadata":{}},{"cell_type":"markdown","source":"### Performing decision tree rule engine visualization, which is a helpful tool to understand your model:","metadata":{}},{"cell_type":"code","source":"# Decision tree Classifier\nclf = DecisionTreeClassifier(criterion='entropy', class_weight=weights_dict, \n                            max_depth=3, max_features=None,\n                            min_samples_split=25, min_samples_leaf=15)\n\n# We fit the model\nclf.fit(X_train, y_train)\n\n# Export now as a dot file\ndot_data = export_graphviz(clf, out_file='tree.dot',\n                          feature_names=selected_feats_dt,\n                          class_names=['Did not churn', 'Churned'],\n                          rounded=True, proportion=False,\n                          precision=2, filled=True)\n# # Convert to png using system command (requires Graphviz installation)\n# subprocess.run(['dot', '-Tpng','tree.dot', '-o', 'tree.png', '-Gdpi=600'], shell=True)\n# # Display the rule-set of a single tree\n# from IPython.display import Image\n# Image(filename='tree.png')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:58.633702Z","iopub.execute_input":"2023-09-23T09:03:58.634063Z","iopub.status.idle":"2023-09-23T09:03:58.672744Z","shell.execute_reply.started":"2023-09-23T09:03:58.634029Z","shell.execute_reply":"2023-09-23T09:03:58.671433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note:\n- In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n- On GitHub, the HTML representation is unable to render, please try loading this page with [nbviewer](https://nbviewer.org/)","metadata":{}},{"cell_type":"markdown","source":"### In the graph above:\n- We create and train a model.\n- The export_graphviz() function creates a GraphViz representation of the decision tree.\n- Using a system command, we can convert the dot to png with the subprocess.run() method command.\n- We visualize the image by calling the function Image(filename = 'tree.png').\n- Entropy is a measure of the unpredictability of the information being processed. The greater the entropy, the more       difficult it is to make inferences from the information.\n- This decision tree reveals that the model is a series of logical questions and answers similar to what humans would develop when making predictions.","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation Automatization and Model Run through Pipelines\n\n### We will define three main classes:\n    \n- Create a sklearn transformer class, namely CategoricalEncoder to perform label encoding, one-hot encoding, and target encoding. We draw our inspiration from this link.\n\n- AddFeatures class that allows us to add newly engineered features using original categorical and numerical features of the DataFrame.\n\n- CustomScaler that can apply scaling on selected columns. Since there is a lot of code, please get the utils.py file here and put it in the working directory.\n\n- After doing grid and randomized searches, we conclude that LGBMClassifier() outperforms all other models.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\" \n    Encodes categorical columns using LabelEncoding, OneHotEncoding and TargetEncoding.\n    LabelEncoding is used for binary categorical columns\n    OneHotEncoding is used for columns with <= 10 distinct values\n    TargetEncoding is used for columns with higher cardinality (>10 distinct values)\n    \n    \"\"\"\n\n    def __init__(self, cols = None, lcols = None, ohecols = None, tcols = None, reduce_df = False):\n        \"\"\"\n        \n        Parameters\n        ----------\n        cols : list of str\n            Columns to encode.  Default is to one-hot/target/label encode all categorical columns in the DataFrame.\n        reduce_df : bool\n            Whether to use reduced degrees of freedom for encoding\n            (that is, add N-1 one-hot columns for a column with N \n            categories). E.g. for a column with categories A, B, \n            and C: When reduce_df is True, A=[1, 0], B=[0, 1],\n            and C=[0, 0].  When reduce_df is False, A=[1, 0, 0], \n            B=[0, 1, 0], and C=[0, 0, 1]\n            Default = False\n        \n        \"\"\"\n        \n        if isinstance(cols,str):\n            self.cols = [cols]\n        else :\n            self.cols = cols\n        \n        if isinstance(lcols,str):\n            self.lcols = [lcols]\n        else :\n            self.lcols = lcols\n        \n        if isinstance(ohecols,str):\n            self.ohecols = [ohecols]\n        else :\n            self.ohecols = ohecols\n        \n        if isinstance(tcols,str):\n            self.tcols = [tcols]\n        else :\n            self.tcols = tcols\n        \n        self.reduce_df = reduce_df\n    \n    \n    def fit(self, X, y):\n        \"\"\"Fit label/one-hot/target encoder to X and y\n        \n        Parameters\n        ----------\n        X : pandas DataFrame, shape [n_samples, n_columns]\n            DataFrame containing columns to encode\n        y : pandas Series, shape = [n_samples]\n            Target values.\n            \n        Returns\n        -------\n        self : encoder\n            Returns self.\n        \"\"\"\n        \n        # Encode all categorical cols by default\n        if self.cols is None:\n            self.cols = [c for c in X if str(X[c].dtype)=='object']\n\n        # Check columns are in X\n        for col in self.cols:\n            if col not in X:\n                raise ValueError('Column \\''+col+'\\' not in X')\n        \n        # Separating out lcols, ohecols and tcols\n        if self.lcols is None:\n            self.lcols = [c for c in self.cols if X[c].nunique() <= 2]\n        \n        if self.ohecols is None:\n            self.ohecols = [c for c in self.cols if ((X[c].nunique() > 2) & (X[c].nunique() <= 10))]\n        \n        if self.tcols is None:\n            self.tcols = [c for c in self.cols if X[c].nunique() > 10]\n        \n        \n        ## Create Label Encoding mapping\n        self.lmaps = dict()\n        for col in self.lcols:\n            self.lmaps[col] = dict(zip(X[col].values, X[col].astype('category').cat.codes.values))\n        \n        \n        ## Create OneHot Encoding mapping\n        self.ohemaps = dict() #dict to store map for each column\n        for col in self.ohecols:\n            self.ohemaps[col] = []\n            uniques = X[col].unique()\n            for unique in uniques:\n                self.ohemaps[col].append(unique)\n            if self.reduce_df:\n                del self.ohemaps[col][-1]\n        \n        \n        ## Create Target Encoding mapping\n        self.global_target_mean = y.mean().round(2)\n        self.sum_count = dict()\n        for col in self.tcols:\n            self.sum_count[col] = dict()\n            uniques = X[col].unique()\n            for unique in uniques:\n                ix = X[col]==unique\n                self.sum_count[col][unique] = (y[ix].sum(),ix.sum())\n        \n        \n        ## Return the fit object\n        return self\n    \n    \n    def transform(self, X, y=None):\n        \"\"\"Perform label/one-hot/target encoding transformation.\n        \n        Parameters\n        ----------\n        X : pandas DataFrame, shape [n_samples, n_columns]\n            DataFrame containing columns to label encode\n            \n        Returns\n        -------\n        pandas DataFrame\n            Input DataFrame with transformed columns\n        \"\"\"\n        \n        Xo = X.copy()\n        ## Perform label encoding transformation\n        for col, lmap in self.lmaps.items():\n            \n            # Map the column\n            Xo[col] = Xo[col].map(lmap)\n            Xo[col].fillna(-1, inplace=True) ## Filling new values with -1\n        \n        \n        ## Perform one-hot encoding transformation\n        for col, vals in self.ohemaps.items():\n            for val in vals:\n                new_col = col+'_'+str(val)\n                Xo[new_col] = (Xo[col]==val).astype('uint8')\n            del Xo[col]\n        \n        \n        ## Perform LOO target encoding transformation\n        # Use normal target encoding if this is test data\n        if y is None:\n            for col in self.sum_count:\n                vals = np.full(X.shape[0], np.nan)\n                for cat, sum_count in self.sum_count[col].items():\n                    vals[X[col]==cat] = (sum_count[0]/sum_count[1]).round(2)\n                Xo[col] = vals\n                Xo[col].fillna(self.global_target_mean, inplace=True) # Filling new values by global target mean\n\n        # LOO target encode each column\n        else:\n            for col in self.sum_count:\n                vals = np.full(X.shape[0], np.nan)\n                for cat, sum_count in self.sum_count[col].items():\n                    ix = X[col]==cat\n                    if sum_count[1] > 1:\n                        vals[ix] = ((sum_count[0]-y[ix].reshape(-1,))/(sum_count[1]-1)).round(2)\n                    else :\n                        vals[ix] = ((y.sum() - y[ix])/(X.shape[0] - 1)).round(2) # Catering to the case where a particular \n                                                                                 # category level occurs only once in the dataset\n                \n                Xo[col] = vals\n                Xo[col].fillna(self.global_target_mean, inplace=True) # Filling new values by global target mean\n        \n        \n        ## Return encoded DataFrame\n        return Xo\n    \n    \n    def fit_transform(self, X, y=None):\n        \"\"\"Fit and transform the data via label/one-hot/target encoding.\n        \n        Parameters\n        ----------\n        X : pandas DataFrame, shape [n_samples, n_columns]\n            DataFrame containing columns to encode\n        y : pandas Series, shape = [n_samples]\n            Target values (required!).\n\n        Returns\n        -------\n        pandas DataFrame\n            Input DataFrame with transformed columns\n        \"\"\"\n        \n        return self.fit(X, y).transform(X, y)\n    \n\n\nclass AddFeatures(BaseEstimator):\n    \"\"\"\n    Add new, engineered features using original categorical and numerical features of the DataFrame\n    \"\"\"\n    \n    def __init__(self, eps = 1e-6):\n        \"\"\"\n        Parameters\n        ----------\n        eps : A small value to avoid divide by zero error. Default value is 0.000001\n        \"\"\"\n        \n        self.eps = eps\n    \n    \n    def fit(self, X, y=None):\n        return self\n    \n    \n    def transform(self, X):\n        \"\"\"\n        Parameters\n        ----------\n        X : pandas DataFrame, shape [n_samples, n_columns]\n            DataFrame containing base columns using which new interaction-based features can be engineered\n        \"\"\"\n        Xo = X.copy()\n        ## Add 4 new columns - bal_per_product, bal_by_est_salary, tenure_age_ratio, age_surname_mean_churn\n        Xo['bal_per_product'] = Xo.Balance/(Xo.NumOfProducts + self.eps)\n        Xo['bal_by_est_salary'] = Xo.Balance/(Xo.EstimatedSalary + self.eps)\n        Xo['tenure_age_ratio'] = Xo.Tenure/(Xo.Age + self.eps)\n        Xo['age_surname_enc'] = np.sqrt(Xo.Age) * Xo.Surname_enc\n        \n        ## Returning the updated dataframe\n        return Xo\n    \n    \n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Parameters\n        ----------\n        X : pandas DataFrame, shape [n_samples, n_columns]\n            DataFrame containing base columns using which new interaction-based features can be engineered\n        \"\"\"\n        return self.fit(X,y).transform(X)\n    \n    \n\nclass CustomScaler(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A custom standard scaler class with the ability to apply scaling on selected columns\n    \"\"\"\n    \n    def __init__(self, scale_cols = None):\n        \"\"\"\n        Parameters\n        ----------\n        scale_cols : list of str\n            Columns on which to perform scaling and normalization. Default is to scale all numerical columns\n        \n        \"\"\"\n        self.scale_cols = scale_cols\n    \n    \n    def fit(self, X, y=None):\n        \"\"\"\n        Parameters\n        ----------\n        X : pandas DataFrame, shape [n_samples, n_columns]\n            DataFrame containing columns to scale\n        \"\"\"\n        \n        # Scaling all non-categorical columns if user doesn't provide the list of columns to scale\n        if self.scale_cols is None:\n            self.scale_cols = [c for c in X if ((str(X[c].dtype).find('float') != -1) or (str(X[c].dtype).find('int') != -1))]\n        \n     \n        ## Create mapping corresponding to scaling and normalization\n        self.maps = dict()\n        for col in self.scale_cols:\n            self.maps[col] = dict()\n            self.maps[col]['mean'] = np.mean(X[col].values).round(2)\n            self.maps[col]['std_dev'] = np.std(X[col].values).round(2)\n        \n        # Return fit object\n        return self\n    \n    \n    def transform(self, X):\n        \"\"\"\n        Parameters\n        ----------\n        X : pandas DataFrame, shape [n_samples, n_columns]\n            DataFrame containing columns to scale\n        \"\"\"\n        Xo = X.copy()\n        \n        ## Map transformation to respective columns\n        for col in self.scale_cols:\n            Xo[col] = (Xo[col] - self.maps[col]['mean']) / self.maps[col]['std_dev']\n        \n        \n        # Return scaled and normalized DataFrame\n        return Xo\n    \n    \n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Parameters\n        ----------\n        X : pandas DataFrame, shape [n_samples, n_columns]\n            DataFrame containing columns to scale\n        \"\"\"\n        # Fit and return transformed dataframe\n        return self.fit(X).transform(X)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:58.674904Z","iopub.execute_input":"2023-09-23T09:03:58.675358Z","iopub.status.idle":"2023-09-23T09:03:58.737098Z","shell.execute_reply.started":"2023-09-23T09:03:58.675321Z","shell.execute_reply":"2023-09-23T09:03:58.73542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install utils\nfrom utils import *\n\n# Preparing data and a few common model parameters\n# Unscaled features will be used since it's a tree model\n\nX_train = dc_train.drop(columns = ['Exited'], axis=1)\nX_val = dc_val.drop(columns = ['Exited'], axis=1)\n\nbest_f1_lgb = LGBMClassifier(boosting_type='dart', class_weight={0: 1, 1: 3.0}, min_child_samples=20, n_jobs=-1, importance_type='gain', max_depth=6, num_leaves=63, colsample_bytree=0.6, learning_rate=0.1, n_estimators=201, reg_alpha=1, reg_lambda=1)\nbest_recall_lgb = LGBMClassifier(boosting_type='dart', num_leaves=31, max_depth=6, learning_rate=0.1, n_estimators=21, class_weight={0: 1, 1: 3.93}, min_child_samples=2, colsample_bytree=0.6, reg_alpha=0.3, reg_lambda=1.0, n_jobs=-1, importance_type='gain')\nmodel = Pipeline(steps = [('categorical_encoding', CategoricalEncoder()),\n                          ('add_new_features', AddFeatures()),\n                          ('classifier', best_f1_lgb)\n                         ])\n\n# Fitting final model on train dataset\nmodel.fit(X_train, y_train)\n\n# Predict target probabilities\nval_probs = model.predict_proba(X_val)[:,1]\n\n# Predict target values on val data\nval_preds = np.where(val_probs > 0.45, 1, 0) # The probability threshold can be tweaked\n\n# Validation metrics\nprint(f'Confusion Matrix: {confusion_matrix(y_val,val_preds)}')\nprint(f'Area Under Curve: {roc_auc_score(y_val,val_preds)}')\nprint(f'Recall score: {recall_score(y_val,val_preds)}')\nprint(f'Classification report: \\n{classification_report(y_val,val_preds)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:58.739279Z","iopub.execute_input":"2023-09-23T09:03:58.739762Z","iopub.status.idle":"2023-09-23T09:04:16.23397Z","shell.execute_reply.started":"2023-09-23T09:03:58.73972Z","shell.execute_reply":"2023-09-23T09:04:16.232567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving the Optimal Model","metadata":{}},{"cell_type":"code","source":"# Save the model using joblib\njoblib.dump(model, 'final_churn_model_f1_0_45.sav')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:04:16.236604Z","iopub.execute_input":"2023-09-23T09:04:16.237611Z","iopub.status.idle":"2023-09-23T09:04:16.265939Z","shell.execute_reply.started":"2023-09-23T09:04:16.237551Z","shell.execute_reply":"2023-09-23T09:04:16.264822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting the Churning Users on the Test Set","metadata":{}},{"cell_type":"code","source":"# Load model object\nmodel = joblib.load('final_churn_model_f1_0_45.sav')\nX_test = dc_test.drop(columns=['Exited'], axis=1)\n\n# Predict target probabilities\ntest_probs = model.predict_proba(X_test)[:,1]\n\n# Predict target values on test data\ntest_preds = np.where(test_probs > 0.45, 1, 0) # Flexibility to tweak the probability threshold\n#test_preds = model.predict(X_test)\n\n# Test set metrics\nroc_auc_score(y_test, test_preds)\nrecall_score(y_test, test_preds)\nconfusion_matrix(y_test, test_preds)\nprint(classification_report(y_test, test_preds))","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:04:16.270982Z","iopub.execute_input":"2023-09-23T09:04:16.272259Z","iopub.status.idle":"2023-09-23T09:04:16.330165Z","shell.execute_reply.started":"2023-09-23T09:04:16.2722Z","shell.execute_reply":"2023-09-23T09:04:16.328867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding predictions and their probabilities in the original test dataframe","metadata":{}},{"cell_type":"code","source":"test = dc_test.copy()\ntest['predictions'] = test_preds\ntest['pred_probabilities'] = test_probs\ntest.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:04:16.331881Z","iopub.execute_input":"2023-09-23T09:04:16.333015Z","iopub.status.idle":"2023-09-23T09:04:16.36725Z","shell.execute_reply.started":"2023-09-23T09:04:16.33296Z","shell.execute_reply":"2023-09-23T09:04:16.365984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Listing customers who have a churn probability higher than 70% sorted by probability, these are the ones who can be targeted immediately:","metadata":{}},{"cell_type":"code","source":"high_churn_list = test[test.pred_probabilities > 0.7].sort_values(by=['pred_probabilities'], ascending=False\n                                                                 ).reset_index().drop(columns=['index', 'Exited', 'predictions'], axis=1)\nhigh_churn_list.shape\nhigh_churn_list.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:04:16.369102Z","iopub.execute_input":"2023-09-23T09:04:16.369561Z","iopub.status.idle":"2023-09-23T09:04:16.407479Z","shell.execute_reply.started":"2023-09-23T09:04:16.369523Z","shell.execute_reply":"2023-09-23T09:04:16.406105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"high_churn_list.to_csv('high_churn_list.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:04:16.410611Z","iopub.execute_input":"2023-09-23T09:04:16.411127Z","iopub.status.idle":"2023-09-23T09:04:16.429366Z","shell.execute_reply.started":"2023-09-23T09:04:16.411076Z","shell.execute_reply":"2023-09-23T09:04:16.427999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n- A prioritization matrix can be defined based on business requirements, wherein specific segments of customers are targeted first. These segments can be determined based on insights through data or the business teams' needs. For example, males who are active members, have a credit card, and are from Germany can be prioritized first because the business potentially sees the max ROI from them.","metadata":{}},{"cell_type":"markdown","source":"### The model can be expanded to predict when a customer will churn. It will further help sales/customer service teams to reduce churn rates by targeting the right customers at the right time.","metadata":{}}]}